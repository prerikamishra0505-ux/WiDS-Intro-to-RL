{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the cell below to get familiarized with classes in python which is required for the assignment\n",
    "#### You can skip this if you're already comfortable with classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to get familiarized with how classes work in python\n",
    "# A 'Class' is like a blueprint. An 'Object' is the actual thing you build from that blueprint.\n",
    "\n",
    "class Hero:\n",
    "    # The __init__ method is the 'Constructor'.\n",
    "    # It sets up the initial stats for every new object (a hero in this case) you create.\n",
    "    # 'self' refers to the specific object (hero) we are talking to right now.\n",
    "    def __init__(self, name, health):\n",
    "        self.name = name          # Attribute: A string to store the name\n",
    "        self.hp = health          # Attribute: An integer for health points\n",
    "        self.inventory = []       # Attribute: A list that starts empty for every hero\n",
    "        self.is_alive = True      # Attribute: A boolean to track status\n",
    "        \n",
    "        print(f\"-> Created {self.name} with {self.hp} HP.\")\n",
    "\n",
    "    # This is a 'Method'. It's a function that only this class can use.\n",
    "    # Notice 'self' must be the first argument so the hero can access its own HP.\n",
    "    def take_damage(self, amount):\n",
    "        self.hp -= amount\n",
    "        print(f\" ! {self.name} took {amount} damage. Current HP: {self.hp}\")\n",
    "        \n",
    "        if self.hp <= 0:\n",
    "            self.is_alive = False\n",
    "            print(f\" x {self.name} has died.\")\n",
    "\n",
    "    # Another method that updates the hero's internal list\n",
    "    def find_item(self, item):\n",
    "        self.inventory.append(item)\n",
    "        print(f\" + {self.name} found a {item}!\")\n",
    "\n",
    "# --- HOW TO USE THE CLASS ---\n",
    "\n",
    "# 1. We create an 'Instance' (the object) by calling the class name.\n",
    "# This automatically runs the __init__ function.\n",
    "player = Hero(\"Arthur\", 100)\n",
    "\n",
    "# 2. We access data inside the object using 'dot notation' (object.variable)\n",
    "print(f\"Starting stats: Name={player.name}, Alive={player.is_alive}\")\n",
    "\n",
    "# 3. We perform actions by calling the methods we defined above.\n",
    "player.take_damage(25)\n",
    "player.find_item(\"Shield\")\n",
    "player.find_item(\"Health Potion\")\n",
    "\n",
    "# 4. The object 'remembers' its state. Let's look at the inventory we built:\n",
    "print(f\"Final Inventory for {player.name}: {player.inventory}\")\n",
    "\n",
    "# 5. Every object is independent. Creating 'player2' won't affect 'player'!\n",
    "player2 = Hero(\"Morgana\", 80)\n",
    "print(f\"Is {player.name} still at 100 HP? No, it is {player.hp}\")\n",
    "print(f\"Is {player2.name} at 100 HP? No, it is {player2.hp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Multi-Armed Bandits\n",
    "\n",
    "In this assignment, you will implement and compare three classic algorithms for the Multi-Armed Bandit (MAB) problem:\n",
    "1. **epsilon-Greedy Algorithm**\n",
    "2. **Upper Confidence Bound (UCB) Algorithm**\n",
    "3. **Thompson Sampling**\n",
    "\n",
    "You are provided with the environment classes (`Arm` and `MultiBandit`). Your task is to fill in the logic for each algorithm class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Arm:\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "    \n",
    "    def pull(self):\n",
    "        return np.random.binomial(1, self.p)\n",
    "\n",
    "class MultiBandit:\n",
    "    def __init__(self, probs = [0.1, 0.2, 0.7, 0.5]):\n",
    "        self.__arms = [Arm(p) for p in probs]\n",
    "        self.__regret = 0\n",
    "        self.__maxp = max(probs)\n",
    "\n",
    "    def num_arms(self):\n",
    "        return len(self.__arms)\n",
    "\n",
    "    def pull(self, arm_num):\n",
    "        reward = self.__arms[arm_num].pull()\n",
    "        self.__regret += self.__maxp - self.__arms[arm_num].p\n",
    "        return reward\n",
    "    \n",
    "    def regret(self):\n",
    "        return self.__regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. epsilon-Greedy Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyAlgorithm:\n",
    "    def __init__(self, num_arms, horizon, epsilon):\n",
    "        self.num_arms = num_arms\n",
    "        self.horizon = horizon\n",
    "        self.epsilon = epsilon\n",
    "        self.arm_pulls = np.zeros(num_arms)\n",
    "        self.arm_rewards = np.zeros(num_arms)\n",
    "        self.regrets = np.zeros(horizon)\n",
    "        # You might want an array to store average rewards\n",
    "\n",
    "    def give_best_arm(self):\n",
    "        # TODO: Return the index of the arm with the highest estimated average reward\n",
    "        pass\n",
    "\n",
    "    def select_arm(self):\n",
    "        # TODO: Implement the epsilon-greedy selection logic\n",
    "        # Ensure each arm is pulled at least once to avoid division by zero\n",
    "        pass\n",
    "\n",
    "    def run_algorithm(self, bandit):\n",
    "        for t in range(self.horizon):\n",
    "            arm_to_pull = self.select_arm()\n",
    "            reward = bandit.pull(arm_to_pull)\n",
    "            \n",
    "            # TODO: Update your internal state (pull counts, rewards, and total regrets)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upper Confidence Bound (UCB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCBAlgorithm:\n",
    "    def __init__(self, num_arms, horizon):\n",
    "        self.num_arms = num_arms\n",
    "        self.horizon = horizon\n",
    "        self.timestep = 0\n",
    "        self.arm_pulls = np.zeros(num_arms)\n",
    "        self.arm_rewards = np.zeros(num_arms)\n",
    "        self.regrets = np.zeros(horizon)\n",
    "\n",
    "    def select_arm(self):\n",
    "        # TODO: Implement UCB selection logic\n",
    "        # Pull each arm once initially\n",
    "        pass\n",
    "\n",
    "    def run_algorithm(self, bandit):\n",
    "        for t in range(self.horizon):\n",
    "            arm_to_pull = self.select_arm()\n",
    "            reward = bandit.pull(arm_to_pull)\n",
    "            \n",
    "            # TODO: Update internal state and record current bandit regret\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Thompson Sampling\n",
    "Implement Thompson Sampling using a Beta Distribution as the prior for each arm's success probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSamplingAlgorithm:\n",
    "    def __init__(self, num_arms, horizon):\n",
    "        self.num_arms = num_arms\n",
    "        self.horizon = horizon\n",
    "        self.successes = np.zeros(num_arms)\n",
    "        self.failures = np.zeros(num_arms)\n",
    "        self.regrets = np.zeros(horizon)\n",
    "\n",
    "    def select_arm(self):\n",
    "        # TODO: Sample from Beta(successes + 1, failures + 1) for each arm and pick the max\n",
    "        pass\n",
    "\n",
    "    def run_algorithm(self, bandit):\n",
    "        for t in range(self.horizon):\n",
    "            arm_to_pull = self.select_arm()\n",
    "            reward = bandit.pull(arm_to_pull)\n",
    "            \n",
    "            # TODO: Update success/failure counts and record regret\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison and Visualization\n",
    "Run all three algorithms on a bandit with probabilities `[0.4, 0.1, 0.6, 0.2]` for 250 timesteps and plot their cumulative regret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [0.4, 0.1, 0.6, 0.2]\n",
    "H = 250\n",
    "\n",
    "# TODO: Initialize Bandits and Algorithms\n",
    "# TODO: Run each algorithm\n",
    "# TODO: Plot the 'regrets' array from each algorithm on a single graph\n",
    "\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('Cumulative Regret')\n",
    "plt.title('Comparison of MAB Algorithms')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
